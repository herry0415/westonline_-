import os
import tensorflow as tf
from transformers import BertTokenizerFast, TFBertForMaskedLM, BertConfig
import numpy as np

# 判断是否有GPU
import tensorflow as tf
from tensorflow.python.client import device_lib

gpus = tf.config.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        print("GPU is available.")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)

if 'GPU' in str(device_lib.list_local_devices()):
    batch_size = 256
    epochs = 2*2
else:
    batch_size = 8*1
    epochs = 2

print("Batch size:", batch_size)
print("Epochs:", epochs)

with open("E:\python_files\归档\data\summary.txt", encoding="gb2312", errors='ignore') as f:
    text_lines = f.read().splitlines()

# 2. 创建 Bert tokenizer
tokenizer = BertTokenizerFast.from_pretrained('my_tokenizer')

# batch读取tokenizer
inputs = tokenizer(text_lines, max_length=128, truncation=True, padding='max_length', return_tensors='tf')

inputs, labels = [], []
new_tokens = ['[BEGIN]', '[END]', '8M']
num_new_tokens = tokenizer.add_tokens(new_tokens)
for line in text_lines:
    line = "[BEGIN]" + line + "[END]"
    tokens = tokenizer.tokenize(line, max_length=128, truncation=True, padding='max_length')

    token_ids = tokenizer.convert_tokens_to_ids(tokens)

    # masked的范围是原句子line的15%
    if len(line) < len(token_ids):
        masked_indices = np.random.choice(range(1, len(line)+1), size=int(len(line) * 0.15), replace=False)
    else:
        masked_indices = np.random.choice(range(1, len(token_ids)), size=int(len(token_ids) * 0.15), replace=False)
    # print(f"masked_indices: {masked_indices}")

    labels_line = [-100] * len(token_ids)
    for idx in masked_indices:
        labels_line[idx] = token_ids[idx]
        token_ids[idx] = tokenizer.mask_token_id  # assuming that the ID of "[MASK]" is know
    inputs.append(token_ids)
    labels.append(labels_line)
    
inputs = tf.constant(inputs)
labels = tf.constant(labels)

dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))
dataset = dataset.shuffle(1024).batch(batch_size, drop_remainder=True)

from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler
import math

# 保存最新的3个模型
class SaveLastThreeModelCheckpoint(ModelCheckpoint):
    def __init__(self, filepath, **kwargs):
        super().__init__(filepath, **kwargs)

    def on_epoch_end(self, epoch, logs=None):
        super().on_epoch_end(epoch, logs)
         # 使用os.path.dirname获取文件所在目录
        if len(os.listdir(os.path.dirname(self.filepath))) > 3: 

            all_checkpoints = os.listdir(os.path.dirname(self.filepath))
            all_checkpoints.sort(key=lambda f: int(f.split('.')[1]))
            os.remove(os.path.join(os.path.dirname(self.filepath), all_checkpoints[0]))
            
# # 位置编码
def positional_encoding(position, d_model):
    # 用于计算角度的函数
    def get_angles(pos, i, d_model):
        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))
        return pos * angle_rates

    # 每个元素pos_encoding[i, j]代表着词i的位置j的编码
    angle_rads = get_angles(np.arange(position)[:, np.newaxis],
                            np.arange(d_model)[np.newaxis, :],
                            d_model)
  
    # sin给偶数索引（indices）上的角度编码
    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
  
    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
    
    pos_encoding = angle_rads[np.newaxis, ...]
    
    return tf.cast(pos_encoding, dtype=tf.float32)
    
def scaled_dot_product_attention(q, k, v, mask):
    # query key 相乘获取匹配关系
    matmul_qk = tf.matmul(q, k, transpose_b=True)

    dk = tf.cast(tf.shape(k)[-1], tf.float32)
    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)

    if mask is not None:
        scaled_attention_logits += (mask * -1e9)  
    # 通过softmax获取attention权重
    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)

    output = tf.matmul(attention_weights, v)

    return output, attention_weights
    
def point_wise_feed_forward_network(d_model, dff):
    return tf.keras.Sequential([
        tf.keras.layers.Dense(dff, activation='relu'),
        tf.keras.layers.Dense(d_model)
    ])
class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        self.d_model = d_model

        assert d_model % self.num_heads == 0

        self.depth = d_model // self.num_heads

        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)

        self.dense = tf.keras.layers.Dense(d_model)

    def split_heads(self, x, batch_size):
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])

    def call(self, v, k, q, mask=None):
        batch_size = tf.shape(q)[0]
        # 设置q, k, v的维度
        q = self.wq(q)
        k = self.wk(k)
        v = self.wv(v)
        # 分拆q, k, v的维度
        q = self.split_heads(q, batch_size)
        k = self.split_heads(k, batch_size)
        v = self.split_heads(v, batch_size)

        scaled_attention, attention_weights = scaled_dot_product_attention(
            q, k, v, mask)
        # 将多头维度的输出合并
        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])

        concat_attention = tf.reshape(scaled_attention, 
                                      (batch_size, -1, self.d_model))

        output = self.dense(concat_attention)

        return output, attention_weights
class TransformerEncoderLayer(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads, dff, rate=0.1):
        super(TransformerEncoderLayer, self).__init__()

        self.mha = MultiHeadAttention(d_model, num_heads)
        self.ffn = point_wise_feed_forward_network(d_model, dff)

        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)

        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, x, training, mask):
        # 用ulti-head attention
        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)
        # 前馈网络，用于处理上面的输出
        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)
        ffn_output = self.dropout2(ffn_output, training=training)
        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)

        return out2
class TransformerEncoder(tf.keras.layers.Layer):
    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):
        super(TransformerEncoder, self).__init__()

        self.d_model = d_model
        self.num_layers = num_layers
        # 词嵌入层
        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)
        self.pos_encoding = positional_encoding(maximum_position_encoding, 
                                                self.d_model)

        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff, rate) 
                           for _ in range(num_layers)]

        self.dropout = tf.keras.layers.Dropout(rate)
        
    def call(self, x, training, mask):
        seq_len = tf.shape(x)[1]

        # 添加词嵌入和位置编码
        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)
        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))
        x += self.pos_encoding[:, :seq_len, :]

        x = self.dropout(x, training=training)

        for i in range(self.num_layers):
            x = self.enc_layers[i](x, training, mask)

        return x  # (batch_size, input_seq_len, d_model)
class MyTransformerModel(tf.keras.Model):
    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, maximum_position_encoding, rate=0.1):
        super(MyTransformerModel, self).__init__()
        # 用于编码输入序列
        self.encoder = TransformerEncoder(num_layers, d_model, num_heads, dff, vocab_size, maximum_position_encoding, rate)
        
        self.final_layer = tf.keras.layers.Dense(vocab_size)

    def call(self, x, training=False, mask=None):
        enc_output = self.encoder(x, training, mask)  # (batch_size, inp_seq_len, d_model)
    
        final_output = self.final_layer(enc_output)  # (batch_size, inp_seq_len, vocab_size)

        return final_output
# 路径设置
model_dir = './transformer_model0'

# 定义损失函数
def mlm_loss(y_true, y_pred):
    y_true_masked = tf.boolean_mask(y_true, tf.not_equal(y_true, -100))
    y_pred_masked = tf.boolean_mask(y_pred, tf.not_equal(y_true, -100))
    return tf.nn.sparse_softmax_cross_entropy_with_logits(y_true_masked, y_pred_masked)

# 定义学习率衰减函数
def lr_scheduler(epoch, lr):
    if epoch < 3:
        return lr  # 前5个epochs保持原始学习率
    else:
        return lr * math.exp(-0.1)  # 后续epochs按指数衰减

checkpoint_dir = f'./{model_dir}/checkpoints'
os.makedirs(checkpoint_dir, exist_ok=True)

# 设置保存模型的回调函数
checkpoint_callback = SaveLastThreeModelCheckpoint(
    filepath=os.path.join(checkpoint_dir, 'weights.{epoch:02d}.hdf5'),
    monitor='loss',
    verbose=1,
    save_best_only=False,
    save_weights_only=True,
    mode='auto',
    save_freq='epoch',
    period=1
)
# 定义学习率调度器回调函数
lr_scheduler_callback = LearningRateScheduler(lr_scheduler)
# 路径设置
model_dir = './transformer_model3'
# 不存在，或存在但为空
if not os.path.exists(model_dir) or not os.listdir(model_dir):
    # 如果路径不存在，则进行训练

    # 加载模型
    model = MyTransformerModel(num_layers=2, d_model=512, num_heads=8, dff=1024, vocab_size=len(tokenizer)
                        , maximum_position_encoding=1000)

    # 定义优化器
    optimizer = Adam(learning_rate=1e-3)

    model.compile(optimizer=optimizer, loss=mlm_loss)

    checkpoint_dir = f'/{model_dir}/checkpoints'
    os.makedirs(checkpoint_dir, exist_ok=True)

    
    # 训练模型
    history = model.fit(dataset, epochs=5, callbacks=[checkpoint_callback, lr_scheduler_callback])

    # 保存模型
    model.save(os.path.join(model_dir, 'transformer_model'))
    # 保存tokenizer
    tokenizer.save_pretrained(model_dir)

else:
    # 如果路径存在，则加载已有模型
    model = tf.keras.models.load_model(os.path.join(model_dir, 'transformer_model'), 
                                          custom_objects={'mlm_loss': mlm_loss})
    print("Load model from", model_dir)
model.summary()

text = "[BEGIN]事实证明8M参[MASK]就能做一个差强人意的模型出来。[END]"
inputs = tokenizer([text], max_length=128, truncation=True, padding='max_length', return_tensors='tf')
#tokenizer是一整句话包括补padding等操作，是处理输入用的。 encoder会自动加上sep 和 cls   但是tokenize不会,只是分词，然后
input_ids = inputs["input_ids"]
print(f"first_data[0].shape={input_ids.shape}")
print(f"type first_data[0]={type(input_ids)}")
outputs = model.predict(input_ids)
outputs = tf.argmax(outputs,axis=-1) 
output = tokenizer.decode(outputs[0])
print(output)
# output只是对mask_index的预测

mask_token_indices = tf.where(input_ids[0] == tokenizer.mask_token_id)

for mask_token_index in mask_token_indices:
    print(f"mask_token_index={mask_token_index}")
    outputs = model.predict(input_ids)
    predicted_token_index = tf.argmax(outputs[0, mask_token_index[0]]).numpy()

    mask_token_index_2d = tf.reshape(mask_token_index, [1, 1]) # 转换成2d tensor以符合tf.tensor_scatter_nd_update的输入要求
    predicted_token_index_1d = tf.reshape(predicted_token_index, [1]) # 转换成1d tensor以符合tf.tensor_scatter_nd_update的输入要求
    predicted_token_index_1d = tf.cast(predicted_token_index_1d, dtype=tf.int32) # 将类型从int64转换为int32

    input_ids = tf.tensor_scatter_nd_update(input_ids[0], mask_token_index_2d, predicted_token_index_1d)
    input_ids = tf.expand_dims(input_ids, 0) # 重新扩展维度以符合模型的输入要求
    '''
    tf.tensor_scatter_nd_update 函数的作用是根据提供的索引和值，更新给定张量的特定位置的元素。它接受三个参数
    第一个：要改变的张量
    第二个：shape(1,1) 补充的位置
    第三个：shape(1,)要替换的值
    '''
predicted_sentence = tokenizer.decode(input_ids[0])
predicted_sentence = predicted_sentence.replace("[CLS] ", "").replace(" [SEP]", "").replace(" [PAD]", "")
# encoder会自动加上sep 和 cls   但是tokenize不会,只是分词，然后
predicted_sentence = predicted_sentence.replace(" ", "")
print(predicted_sentence)
